{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "\n",
    "import json\n",
    "\n",
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ScanNet scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = \"scene0000_00\"\n",
    "\n",
    "scan_path = os.path.join(current_directory, \"data/scans\", scan)\n",
    "scene_mesh_path = os.path.join(scan_path, scan + \"_vh_clean_2.ply\")\n",
    "scene_mesh = o3d.io.read_triangle_mesh(scene_mesh_path)\n",
    "segmentation_mesh_path = os.path.join(scan_path, scan + \"_vh_clean_2.labels.ply\")\n",
    "segmentation_mesh = o3d.io.read_triangle_mesh(segmentation_mesh_path)\n",
    "\n",
    "# o3d.visualization.draw_geometries([scene_mesh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class must be chosen among one of the raw_category in the [scannetv2-labels.combinesd.tsv](data/scannetv2-labels.combined.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All labels in the aggregation data:  {'kitchen cabinets', 'bicycle', 'kitchen counter', 'curtain', 'clock', 'tissue box', 'cabinet', 'window', 'toilet', 'floor', 'trash can', 'table', 'sink', 'toaster', 'dish rack', 'shelf', 'toaster oven', 'bed', 'pillow', 'couch', 'mirror', 'laundry basket', 'guitar case', 'stool', 'backpack', 'desk', 'ceiling', 'door', 'wall', 'microwave', 'nightstand', 'coffee table', 'object', 'doorframe', 'scale', 'refrigerator', 'guitar', 'shoes', 'tv', 'shower'}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(scan_path, scan + '_vh_clean.aggregation.json')) as f:\n",
    "    aggregation_data = json.load(f)\n",
    "\n",
    "all_classes = set()\n",
    "for seg_group in aggregation_data['segGroups']:\n",
    "    all_classes.add(seg_group['label'])\n",
    "\n",
    "print(\"All labels in the aggregation data: \", all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 objects for class bed: [37]\n"
     ]
    }
   ],
   "source": [
    "class_name = 'bed'\n",
    "\n",
    "with open(os.path.join(scan_path, scan + '_vh_clean.aggregation.json')) as f:\n",
    "    aggregation_data = json.load(f)\n",
    "\n",
    "object_ids = []\n",
    "for seg_group in aggregation_data['segGroups']:\n",
    "    if seg_group['label'] == class_name:\n",
    "        object_ids.append(seg_group['objectId'])\n",
    "\n",
    "if not object_ids:\n",
    "    print(\"No objects found for class:\", class_name)\n",
    "else:\n",
    "    print(f\"Found {len(object_ids)} objects for class {class_name}: {object_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = 32\n",
    "for seg_group in aggregation_data['segGroups']:\n",
    "    if seg_group['objectId'] == object_id:\n",
    "        selected_instance_segments = seg_group['segments']\n",
    "\n",
    "# Load all vertices\n",
    "with open(os.path.join(scan_path, scan + '_vh_clean_2.0.010000.segs.json')) as f:\n",
    "    segmentation_data = json.load(f)\n",
    "\n",
    "# Load instance vertices\n",
    "all_vertices = np.array(segmentation_data['segIndices'])\n",
    "\n",
    "# Get indices of instance vertices\n",
    "instance_vertices_mask = np.isin(all_vertices, selected_instance_segments)\n",
    "instance_vertex_indices = np.where(instance_vertices_mask)[0]\n",
    "\n",
    "# Filter faces of the mesh: included if all its vertices are part of the instance\n",
    "faces = np.asarray(scene_mesh.triangles)\n",
    "face_mask = np.all(np.isin(faces, instance_vertex_indices), axis=1)\n",
    "instance_faces = faces[face_mask] # Triplets of vertex indices forming each triangle (the indices refer to scene vertices - all of them)\n",
    "instance_vertices = np.asarray(scene_mesh.vertices)[instance_vertex_indices] # Coordinates of each vertex\n",
    "vertex_remap = {scene_idx: instance_idx for instance_idx, scene_idx in enumerate(instance_vertex_indices)}\n",
    "instance_faces = np.vectorize(vertex_remap.get)(instance_faces) # Triplets of vertex indices forming each triangle (the indices refer to instance vertices - masked)\n",
    "\n",
    "# Create the mesh for the selected instance\n",
    "instance_mesh = o3d.geometry.TriangleMesh()\n",
    "instance_mesh.vertices = o3d.utility.Vector3dVector(instance_vertices)\n",
    "instance_mesh.triangles = o3d.utility.Vector3iVector(instance_faces)\n",
    "\n",
    "# o3d.visualization.draw_geometries([instance_mesh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get camera parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export camera parameters\n",
    "**Note:** This step should be performed only at the beginning of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(filename='/Users/lara/Desktop/Making-CLIP-features-multiview-consistent/data/scans/scene0000_00/scene0000_00.sens', output_path='/Users/lara/Desktop/Making-CLIP-features-multiview-consistent/outputs/reader/scene0000_00', export_depth_images=False, export_color_images=True, export_poses=True, export_intrinsics=True)\n",
      "loading /Users/lara/Desktop/Making-CLIP-features-multiview-consistent/data/scans/scene0000_00/scene0000_00.sens...loaded!\n",
      "exporting 5578 color frames to /Users/lara/Desktop/Making-CLIP-features-multiview-consistent/outputs/reader/scene0000_00/color\n"
     ]
    }
   ],
   "source": [
    "reader_directory = os.path.join(current_directory, \"scripts/SensReader\")\n",
    "sens_file = os.path.join(scan_path, scan + \".sens\")\n",
    "output_directory = os.path.join(current_directory, \"outputs/reader/\"+scan)\n",
    "\n",
    "os.system(f\"python {os.path.join(reader_directory, 'reader.py')} --filename {sens_file} --output_path {output_directory} --export_color_images --export_poses --export_intrinsics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load camera parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.join(current_directory, \"outputs/reader/\"+scan)\n",
    "pose_directory = os.path.join(output_directory, \"pose\")\n",
    "originals_directory = os.path.join(output_directory, \"color\")\n",
    "pose_files = [f for f in os.listdir(pose_directory) if f.endswith('.txt')]\n",
    "frame_indices = [int(f.split('.')[0]) for f in pose_files]\n",
    "intrinsics = np.loadtxt(os.path.join(output_directory, \"intrinsic\", \"intrinsic_color.txt\"))  # Camera intrinsics\n",
    "extrinsics = np.loadtxt(os.path.join(output_directory, \"intrinsic\", \"extrinsic_color.txt\"))  # Camera estrinsic\n",
    "camera_intrinsics = intrinsics[:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select frames that contain the specific object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select minimum_vertices in an image\n",
    "Select the percentage of the object you want in the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 60\n",
    "\n",
    "total_vertices = instance_vertices.shape[0]\n",
    "minimum_vertices = int(percentage / 100 * total_vertices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get masked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frames = []\n",
    "masked_images_directory = os.path.join(current_directory, \"outputs/masked_images/project_with_occlusion/\"+scan, f\"{object_id}\")\n",
    "if not os.path.exists(masked_images_directory):\n",
    "    os.makedirs(masked_images_directory)\n",
    "\n",
    "\n",
    "def project_vertices(points_3d, intrinsic, extrinsic):\n",
    "    points_3d_homogeneous = np.hstack([points_3d, np.ones((points_3d.shape[0], 1))])\n",
    "    points_camera = np.dot(extrinsic, points_3d_homogeneous.T).T\n",
    "    points_camera_xyz = points_camera[:, :3] # Extract only the x, y, z components because the intrinsic matrix is 3x3\n",
    "    points_2d_homogeneous = np.dot(intrinsic, points_camera_xyz.T).T # the third value is the depth\n",
    "    positive_depth_indices = points_2d_homogeneous[:, 2] > 0 # select only points in front of the camera\n",
    "    points_2d_homogeneous_positive = points_2d_homogeneous[positive_depth_indices]\n",
    "    points_2d = points_2d_homogeneous_positive[:, :2] / points_2d_homogeneous_positive[:, 2, np.newaxis]\n",
    "    return points_2d\n",
    "\n",
    "def draw_triangle(img, vertices, color):\n",
    "    cv2.fillConvexPoly(img, vertices, color)\n",
    "\n",
    "example_img = cv2.imread(os.path.join(originals_directory, \"0.jpg\")) # image used to get the image shape\n",
    "image_height, image_width = example_img.shape[:2]\n",
    "\n",
    "for frame_index in frame_indices:\n",
    "    pose_path = os.path.join(pose_directory, f\"{frame_index}.txt\")\n",
    "    original_path = os.path.join(originals_directory, f\"{frame_index}.jpg\")\n",
    "    pose = np.loadtxt(pose_path)\n",
    "    original = cv2.imread(original_path)\n",
    "    camera_extrinsics = np.linalg.inv(pose)\n",
    "\n",
    "    object_vertices_2d = project_vertices(np.asarray(instance_mesh.vertices), camera_intrinsics, camera_extrinsics)\n",
    "    in_bounds_mask = (object_vertices_2d[:, 0] >= 0) & (object_vertices_2d[:, 0] < image_width) & (object_vertices_2d[:, 1] >= 0) & (object_vertices_2d[:, 1] < image_height)\n",
    "    in_bounds_points_2d = object_vertices_2d[in_bounds_mask]\n",
    "\n",
    "    if in_bounds_points_2d.shape[0] >= minimum_vertices:\n",
    "        selected_frames.append(frame_index)\n",
    "        binary_mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "        for face in instance_mesh.triangles:\n",
    "            pts_2d = np.array(object_vertices_2d[face], dtype=np.int32)\n",
    "            draw_triangle(binary_mask, pts_2d, 1)\n",
    "\n",
    "        masked_image = cv2.bitwise_and(original, original, mask=binary_mask)\n",
    "        output_path = os.path.join(masked_images_directory, f\"{frame_index:05d}.jpg\")\n",
    "        cv2.imwrite(output_path, masked_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical-work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
